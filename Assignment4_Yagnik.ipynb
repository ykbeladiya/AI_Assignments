{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0efdeb3c-40c7-4a1e-b425-c6a4e609295d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Training model with ReLU, optimizer=Adam, lr=0.001\n",
      "Epoch 1: Train Loss: 3.5901, Validation Accuracy: 25.24%\n",
      "Epoch 2: Train Loss: 2.7982, Validation Accuracy: 31.22%\n",
      "Training model with ReLU, optimizer=Adam, lr=0.01\n",
      "Epoch 1: Train Loss: 4.6206, Validation Accuracy: 1.04%\n",
      "Epoch 2: Train Loss: 4.6088, Validation Accuracy: 0.88%\n",
      "Training model with ReLU, optimizer=SGD, lr=0.001\n",
      "Epoch 1: Train Loss: 4.6048, Validation Accuracy: 1.14%\n",
      "Epoch 2: Train Loss: 4.6010, Validation Accuracy: 1.66%\n",
      "Training model with ReLU, optimizer=SGD, lr=0.01\n",
      "Epoch 1: Train Loss: 4.5723, Validation Accuracy: 3.53%\n",
      "Epoch 2: Train Loss: 4.2419, Validation Accuracy: 9.30%\n",
      "Training model with LeakyReLU, optimizer=Adam, lr=0.001\n",
      "Epoch 1: Train Loss: 3.5349, Validation Accuracy: 25.59%\n",
      "Epoch 2: Train Loss: 2.6910, Validation Accuracy: 33.62%\n",
      "Training model with LeakyReLU, optimizer=Adam, lr=0.01\n",
      "Epoch 1: Train Loss: 4.6306, Validation Accuracy: 3.54%\n",
      "Epoch 2: Train Loss: 4.1472, Validation Accuracy: 12.03%\n",
      "Training model with LeakyReLU, optimizer=SGD, lr=0.001\n",
      "Epoch 1: Train Loss: 4.6050, Validation Accuracy: 1.16%\n",
      "Epoch 2: Train Loss: 4.6021, Validation Accuracy: 1.46%\n",
      "Training model with LeakyReLU, optimizer=SGD, lr=0.01\n",
      "Epoch 1: Train Loss: 4.5800, Validation Accuracy: 4.05%\n",
      "Epoch 2: Train Loss: 4.2778, Validation Accuracy: 9.70%\n",
      "Training model with ELU, optimizer=Adam, lr=0.001\n",
      "Epoch 1: Train Loss: 3.2754, Validation Accuracy: 30.62%\n",
      "Epoch 2: Train Loss: 2.4255, Validation Accuracy: 35.65%\n",
      "Training model with ELU, optimizer=Adam, lr=0.01\n",
      "Epoch 1: Train Loss: 5.3918, Validation Accuracy: 1.00%\n",
      "Epoch 2: Train Loss: 5.2706, Validation Accuracy: 0.96%\n",
      "Training model with ELU, optimizer=SGD, lr=0.001\n",
      "Epoch 1: Train Loss: 4.5917, Validation Accuracy: 2.52%\n",
      "Epoch 2: Train Loss: 4.5577, Validation Accuracy: 4.77%\n",
      "Training model with ELU, optimizer=SGD, lr=0.01\n",
      "Epoch 1: Train Loss: 4.3977, Validation Accuracy: 8.82%\n",
      "Epoch 2: Train Loss: 3.8831, Validation Accuracy: 12.98%\n",
      "Training model with Sigmoid, optimizer=Adam, lr=0.001\n",
      "Epoch 1: Train Loss: 4.3000, Validation Accuracy: 9.36%\n",
      "Epoch 2: Train Loss: 3.7455, Validation Accuracy: 14.75%\n",
      "Training model with Sigmoid, optimizer=Adam, lr=0.01\n",
      "Epoch 1: Train Loss: 4.9453, Validation Accuracy: 0.80%\n",
      "Epoch 2: Train Loss: 4.9308, Validation Accuracy: 1.15%\n",
      "Training model with Sigmoid, optimizer=SGD, lr=0.001\n",
      "Epoch 1: Train Loss: 4.6308, Validation Accuracy: 1.07%\n",
      "Epoch 2: Train Loss: 4.6138, Validation Accuracy: 0.98%\n",
      "Training model with Sigmoid, optimizer=SGD, lr=0.01\n",
      "Epoch 1: Train Loss: 4.6130, Validation Accuracy: 0.96%\n",
      "Epoch 2: Train Loss: 4.6084, Validation Accuracy: 0.94%\n",
      "Retraining and testing top model with optimizer=Adam, lr=0.001\n",
      "Full Training Epoch 1: Train Loss: 2.0526\n",
      "Full Training Epoch 2: Train Loss: 1.6583\n",
      "Test accuracy for model with optimizer=Adam, lr=0.001: 40.76%\n",
      "Retraining and testing top model with optimizer=Adam, lr=0.001\n",
      "Full Training Epoch 1: Train Loss: 2.3139\n",
      "Full Training Epoch 2: Train Loss: 1.9789\n",
      "Test accuracy for model with optimizer=Adam, lr=0.001: 40.89%\n",
      "Retraining and testing top model with optimizer=Adam, lr=0.001\n",
      "Full Training Epoch 1: Train Loss: 2.4539\n",
      "Full Training Epoch 2: Train Loss: 2.1869\n",
      "Test accuracy for model with optimizer=Adam, lr=0.001: 39.27%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rates = [0.001, 0.01]\n",
    "epochs = 2\n",
    "\n",
    "# Step 1: Load and preprocess the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load CIFAR-100 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Split training dataset into sub-training and validation sets\n",
    "num_train = len(train_dataset)\n",
    "num_subtrain = int(0.8 * num_train)\n",
    "num_val = num_train - num_subtrain\n",
    "subtrain_dataset, val_dataset = random_split(train_dataset, [num_subtrain, num_val])\n",
    "\n",
    "# Data loaders for sub-training, validation, and test sets\n",
    "subtrain_loader = DataLoader(subtrain_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define a function to create various models\n",
    "def create_model(activation, num_features=256):\n",
    "    class CNNModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(CNNModel, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "            self.act1 = activation()\n",
    "            self.pool = nn.MaxPool2d(2, 2)\n",
    "            self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "            self.act2 = activation()\n",
    "            self.fc1 = nn.Linear(64 * 8 * 8, num_features)\n",
    "            self.act3 = activation()\n",
    "            self.fc2 = nn.Linear(num_features, 100)  # CIFAR-100 has 100 classes\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.pool(self.act1(self.conv1(x)))\n",
    "            x = self.pool(self.act2(self.conv2(x)))\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.act3(self.fc1(x))\n",
    "            x = self.fc2(x)\n",
    "            return x\n",
    "    return CNNModel\n",
    "\n",
    "# Activations and Optimizers to experiment with\n",
    "activations = [nn.ReLU, nn.LeakyReLU, nn.ELU, nn.Sigmoid]\n",
    "optimizers_dict = {'Adam': optim.Adam, 'SGD': optim.SGD}\n",
    "results = []\n",
    "\n",
    "# Experiment with models\n",
    "for activation in activations:\n",
    "    for optimizer_name, Optimizer in optimizers_dict.items():\n",
    "        for lr in learning_rates:\n",
    "            model = create_model(activation)()\n",
    "            print(f\"Training model with {activation.__name__}, optimizer={optimizer_name}, lr={lr}\")\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            opt = Optimizer(model.parameters(), lr=lr)\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            model.to(device)\n",
    "\n",
    "            # Training and Validation\n",
    "            for epoch in range(epochs):\n",
    "                model.train()\n",
    "                train_loss = 0\n",
    "                for images, labels in subtrain_loader:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    opt.zero_grad()\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    opt.step()\n",
    "                    train_loss += loss.item()\n",
    "                avg_train_loss = train_loss / len(subtrain_loader)\n",
    "\n",
    "                model.eval()\n",
    "                total = 0\n",
    "                correct = 0\n",
    "                with torch.no_grad():\n",
    "                    for images, labels in val_loader:\n",
    "                        images, labels = images.to(device), labels.to(device)\n",
    "                        outputs = model(images)\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total += labels.size(0)\n",
    "                        correct += (predicted == labels).sum().item()\n",
    "                accuracy = 100 * correct / total\n",
    "                print(f\"Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, Validation Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "            results.append((model, optimizer_name, lr, accuracy))\n",
    "\n",
    "# Sort and select top 3 models\n",
    "results.sort(key=lambda x: x[3], reverse=True)\n",
    "top_models = results[:3]\n",
    "\n",
    "# Re-train top 3 models on full training dataset and evaluate on test set\n",
    "full_train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_accuracies = []\n",
    "for model, optimizer_name, lr, _ in top_models:\n",
    "    print(f\"Retraining and testing top model with optimizer={optimizer_name}, lr={lr}\")\n",
    "    opt = optimizers_dict[optimizer_name](model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Full training\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for images, labels in full_train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            opt.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            train_loss += loss.item()\n",
    "        avg_train_loss = train_loss / len(full_train_loader)\n",
    "        print(f\"Full Training Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Testing\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    test_accuracy = 100 * correct / total\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    print(f\"Test accuracy for model with optimizer={optimizer_name}, lr={lr}: {test_accuracy}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e812e5-b7e1-430f-a771-8a9c1ac5156b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
